%%%% ijcai24.tex

\typeout{IJCAI--24 Instructions for Authors}

% These are the instructions for authors for IJCAI-24.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai24.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai24}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
% \usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{cleveref}
\usepackage{enumerate}

% Comment out this line in the camera-ready submission
\nolinenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{assumption}{Assumption}[section]

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2024.0)
}

\title{ACsleuth: Domain Adaptive and Fine-grained Anomalous Cell Detection for Single-cell Multiomics}


% Single author syntax
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}

% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
\iffalse
\author{
First Author$^1$
\and
Second Author$^2$\and
Third Author$^{2,3}$\And
Fourth Author$^4$
\affiliations
$^1$First Affiliation\\
$^2$Second Affiliation\\
$^3$Third Affiliation\\
$^4$Fourth Affiliation
\emails
\{first, second\}@example.com,
third@other.example.com,
fourth@example.com
}
\fi


\begin{document}

\maketitle

\begin{abstract}
    Fined-grained anomaly detection at the cellular level is a crucial phase in diagnosing 
    conditions and pathological analyses. Since the single-cell RNA sequencing (scRNA-seq) 
    data analysis has been greatly prompted by the development of deep learning, we can 
    effectively detect anomalous cells and explore their detail types. However, most of 
    works simply focus on detecting anomalies within normal samples, overlooking the 
    opportunity for more detailed distinctions of them. Moreover, domain bias, commonly 
    manifested as batch effects and distribution heterogeneities among various sequencing technologies, 
    presents significant challenges in biological data analysis. Thus, we introduce an innovative workflow called ACsleuth, 
    aiming at domain adaptive and fine-grained anomalous cell detection for single-cell multiomics.
    We employ a GAN-based model to reconstruct normal samples and leveraging the Maximum Mean Discrepancy (MMD) 
    metric for the unbiased anomaly inference. We then utilize the DESC clustering method for 
    scRNA-seq data to explore subtypes of anomalies. Extensive experiments improve the superiority 
    of ACsleuth both of the comparison with the state-of-the-art anomaly detection methods in 
    detecting anomalies and simple combinations of anomaly detection and clustering methods in 
    fine-grained anomalous detection tasks.
\end{abstract}

\section{Introduction}

Recently, single-cell RNA sequencing (scRNA-seq) technologies has developed rapidly, 
which allows us to better explore tissue heterogeneity at the cellular level. It yields 
a gene expression matrix, where each vector represents the expression values of specific 
genes across cells. In addition, detecting diseased cells, commonly referred to anomaly 
detection, constitutes a crucial phase in diagnosing conditions and conducting pathological 
analyses. Moreover, fine-grained detection of diseased cells, which can also be called subtyping, enables a more comprehensive 
understanding of disease classifications and facilitates the development of nuanced and 
efficacious treatment strategies. Therefore, leveraging the gene expression matrix derived 
from scRNA-seq empowers us to enhance detecting anomalous cells, then delve deeper into 
the exploration of their fine-grained subtypes.

Motivated by deep learning, previous studies for scRNA-seq data mainly focus on cell 
clustering and cell type annotation. The former aims to identify cell groups \cite{sczi-Desk,scCNC},
and the latter simply annotates these groups based on marker genes or another prior information \cite{ItClust,scArches}. 
However, both of aforementioned tasks are not ideally suited for fine-grained anomalous cells detection. 
Anomaly detection naturally exhibits class-imbalance characteristics, while the challenge 
in fine-grained anomaly detection lies in acquiring more precise representations of distinct cell types 
due to their inherent similarity. Nevertheless, employing reference-based methods can be challenging, given the presence of 
technical variation (e.g., different laboratory conditions) across diverse studies, 
commonly recognized as batch effects in molecular biology literature \cite{Polyphony}. 
The intricate nature of scRNA-seq data, characterized by noise, batch effects, high 
dimensionality and sparsity \cite{dands} poses challenges that impede these deep 
methods from achieving optimal performance directly based on the raw data.

Domain adaptation is a crucial consideration in biological data analysis. Under the same sequencing technique, one of most 
typical instances is correcting batch effect, as we have discussed above. For example, the gene expression of cells from 
the same tissue but obtained from different patients, may be interfered by pronounced batch effects. 
Additionally, data derived from different sequencing technologies exhibits heterogeneous distributions, while 
compared with scRNA-seq,other sequencing technologies like scATAC-seq are even fewer \cite{RNAmore}.
This dataset shift is commonly denoted as domain bias in computer vision, encompassing aspects 
such as style differences, diverse sensory devices, etc \cite{domainada}.
Both batch effects within the same sequencing technique data and the bias arising from different 
sequencing technologies contribute to domain bias \cite{ACE}. However, most previous studies failed to 
recognize such shared characteristics between these two biases and tended to address them 
independently, which aren't efficient.

Besides, most of anomaly detection studies work solely on identifying anomalies within normal 
samples, which often treating it as a class-imbalance binary classification task \cite{OA}. 
However, these studies overlook the opportunity for more detailed classification of 
anomalies, such as distinguishing between different types of tumors or various criminal 
behaviors. This limitation results in underutilization of the available data and hinders 
the ability to learn more granular representations of the anomalous samples. 
Furthermore, the core objective of both anomaly detection and fine-grained detection is to 
acquire a more nuanced representation for each type of samples. This shared optimalization 
goal suggests that these two tasks can seamlessly integrate into an entire pipeline. 
Starting with raw data encompassing both normal and anomalous samples, the pipeline can 
effectively distinguish anomalies of various subtypes.

To address these limitations, we propose an comprehensive workflow consists of fine-grained anomalous cell 
detection and batch correction for scRNA-seq data, named ACsleuth, aiming at 
subtyping anomalies from the raw data which contains various normal and diseased types of 
cells. Our contributions are summarized as follows:
\begin{itemize}
\item Innovatively, we introduce an conprehensive workflow called ACsleuth, aiming at 
fine-grained anomalous cell detection. The anomaly detector is first trained 
unsupervisedly on the reference dataset which only contains normal samples, then we learn 
the batch effects on the normal samples identified by the former detector and use it for 
the batch correction on anomaly sets sequentially, finally subtyping anomalies simply by a recent 
single-cell clustering method.
\item Methodologically, we utilize Maximum Mean Discrepancy (MMD) for inferring anomalous cells. 
We prove that the MMD metric remains unaffected by domain bias, ensuring effective training and  
detection across different batches and enabling potential cross-domain anomaly detection. We then 
model batch effects using normal cells from both reference and target datasets, followed by batch 
correction applied to detected anomalies before fine-grained subtyping them.
\item Empirically, we conduct extensive experiments to validate the exceptional performance 
of ACsleuth. Across evaluations on three distinct scRNA-seq datasets and one scATAC-seq dataset 
for cross-domain anomaly detection task within different numbers of highly variable genes, 
ACsleuth consistently raised state-of-the-art in the majority of anomaly detection and 
fine-grained detection.
\end{itemize}

\section{Related Works}
\subsection{Anomaly Detection}
Anomaly detection usually relies on learning distinct representations between normal 
samples and anomalies. Many studies focus on representing normal samples and 
anomalies are detected by filtering representations that are dissimilar to normal ones. 
One straightforward manner is one-class classification, which aims to train a model that 
can accurately describe normal samples and then distinguish whether test samples are from 
the same distribution as the reference data. This kind of detectors is trained to learn a 
new representation \cite{RCA,liznerski} that 
enhances the dissimilarity between embeddings of normal and anomalous samples, thereby 
improving the detectability of anomalies. While the one-class assumption is vulnerable 
since real datasets often contain multiple inliers \cite{SLAD}.

Generative models are another standard procedure in detecting outliers. They learn by 
reconstructing normal samples, which leads to poor reconstruction of anomalies in 
the target data due to their distinct reconstruction error. This is attributed to the 
fact that the reference data for the anomaly detection task exclusively consists of 
normal samples.
Popular frameworks such as autoencoder (AE) \cite{odae}, generative adversarial 
networks (GAN) \cite{GANad} are widely used. Nevertheless, such GAN-based 
models like \cite{EffGAN} still struggle with distinguishing multiple normal samples, and are prone to 
occurring model-collapse during training. Additionally, AE frameworks exhibit limitations 
handling the noisy or high-dimensional sparse data.

\subsection{scRNA-seq Data Fine-grained Detection}
Single-cell fine-grained type detection aims to identify distinct subtypes within the same cell type, e.g., 
tumors, which is similar to a broader task called cell annotation. The essence of two 
tasks lies in learning representations that effectively capture the heterogeneity among 
distinct types to the fullest extent. Cell annotation simply follows three 
steps \cite{threestep}: learning a compact representation by projecting cells to a 
lower-dimensional space, mapping similar cells to groups in the low-dimensional 
representation (typically via clustering), and finally characterizing the differences in 
gene expression among the cell groups. ACE \cite{ACE} uses AE to generate 
low-dimensional representations and considers the intrinsic dependencies among genes. 
Kratos \cite{kratos} fuses the dimension reduction and clustering cells to 
optimize jointly. scTAG \cite{scTAG} simultaneously identifies cells clusters and 
learns topological representations between cells. scPOT \cite{scPOT} achieves 
annotating seen cell types and novel cell type clustering simultaneously. For subtype 
detection, SCEVAN \cite{SCEVAN} and CopyKAT \cite{CopyKAT} are designed 
specifically for tumors since they used the prior information, CAMLU \cite{CAMLU} 
exhibits a higher degree of generality via the employment of AE, albeit its performance is suboptimal. 

Detecting fine-grained subtypes solely involves assigning labels to distinct subtypes of observations, 
while cell annotation demands the utilization of prior information to assign specific 
labels to the cells. However, compared to cell annotation, fine-grained subtype detection necessitates 
the acquisition of more robust representations. Given that the observations from different 
subtypes fall under the same subcategory, the inherent differences between them tend to 
be subtler. Therefore, fine-grained subtype detection relies heavily on distinctive representations to 
achieve accurate classification. In summary, both anomaly detection and fine-grained detection 
necessitate learning precise representations, so that it is straightforward that we can 
propose a pipeline comprising anomaly detection and fine-grained subtype detection via the identical objective. 
Notably, to the best of our knowledge, none of the previous works are designed for a 
comprehensive workflow which contains such subtasks including anomaly detection and 
anomalies detailed multi-classification.

\section{Methods}
\begin{figure}
    \centering
    \includegraphics[scale=0.20]{Framework.jpg}
    \caption{Illustration of ACsleuth. The overall model consists of GAN-based anomaly detection and batch effect correction modules and DESC clustering module for fine-grained anomalous cell detection.}
    \label{fig:workflow}
\end{figure}
We first give an overview of subtyping cells workflow as \Cref{fig:workflow}, which can be divided into two main 
parts: anomaly detection and subtype clustering. We start with training the detector 
unsupervisedly, as the reference dataset contains only normal cells. The target dataset 
contains two kinds of labels: the two-class one for distinguishing between normal and 
anomalous cells, and the detailed one for categorizing subtypes of cells. Specifically, 
we exclusively leverage the subtyping module for anomalies discovered by the detector, 
which we refer to as “pre-anomalies”. That means they may include a few “fake anomalies”. 
ACsleuth consists of three modules, each dedicated to anomaly detection, batch effect correction and subtyping.

\subsection{Detecting Anomaly Cells}
ACsleuth detects anomalies based on learning the reconstruction of normal samples in the reference datasets through GAN. 
As ACsleuth is specifically trained to reconstruct normal samples, 
anomalous cells in target datasets are more likely to hold larger reconstruction errors, 
which are treated as an assessment criterion. 

\textbf{Model Training.}
Our GAN module (referred as \textit{module I}) consists of a generator and a discriminator. 
The generator can be subdivided into three distinct components: an encoder, a decoder and a memory block.
To define the gene expression pattern, we employ the mathematical model similar to \cite{hornung2016combining}.
Let $S^k$ denote the group of cells in the same batch $k$.
Then, we propose an assumption as follows.
\begin{assumption}\label{as:pattern}
    The gene expression vector $\bm{x}_i \in S^k \cap \mathbb{R}^{N_{gene}}$ of cell $i$ can be represented as:
    \begin{equation}\label{eq:pattern}
        \bm{x}_i = \bm{x}_i^* + \bm{b}_i^k + \bm{\epsilon}_i
    \end{equation}
    where $\bm{x}_i^*$ denotes the biological factor, $\bm{b}_i^k$ denotes batch-specific factor.
    $\bm{x}_i^* \sim P_{\bm{x}^*}$, $\bm{b}_i^k \sim P_{\bm{b}^k}$, and random noise $\bm{\epsilon}_i\sim N(0, \bm{\sigma}_i^2)$.
\end{assumption}
Thus, we have the embeddings of the cell $i$ with ACsleuth's encoder (MLP backbone, $G_E:\mathbb{R}^{N_{gene}} \rightarrow \mathbb{R}^p$) as:
\begin{equation}
    \bm{z}_i = G_E(\bm{x}_i)
\end{equation}
The memory block is fundamentally an embedding queue $\bm{Q}\in\mathbb{R}^{N_{mem}\times p}$ 
filled with $\bm{z}$, where $N_{mem}$ is the number of in-memory embeddings.
It provides an attention-based means for reconstructing the embedding as $\widetilde{\bm{z}}_i \in \mathbb{R}^p$: 
\begin{equation}
    \widetilde{\bm{z}}_i = \bm{Q}^T \text{softmax}\left(\frac{\bm{Q}\bm{z}_i}{\tau}\right)
\end{equation}
where $\tau$ is the temperature hyperparameter.
During the entire training procedure, $\bm{Q}$ is dynamically updated by enqueuing the most recently 
reconstructed $\widetilde{\bm{z}}$ and dequeuing the oldest ones, thereby striking 
a balance between preserving learnt features and adapting to new samples also 
effectively mitigating the risks of mode collapse. 
Subsequently, the decoder (MLP backbone, $G_D:\mathbb{R}^p \rightarrow \mathbb{R}^{N_{gene}}$) reconstructs the gene expression vectors 
with $\widetilde{\bm{z}}_i$ as
\begin{equation}
    \widehat{\bm{x}}_i = G_D(\widetilde{\bm{z}}_i)
\end{equation}

The discriminator $D: \mathbb{R}^{N_{gene}} \rightarrow \mathbb{R}^k$ is trained to distinguish whether $\bm{x}$ and $\widehat{\bm{x}}$ is real or generated.
Therefore, loss functions of the generator and the discriminator for anomaly detection is defined as:
\begin{equation}
\begin{split}
    \mathcal{L}_{G_1} &= \alpha\mathcal{L}_{\text{rec}} + \beta\mathcal{L}_{\text{adv}} \\
    &= \alpha\mathbb{E}\left[\Vert \bm{x} - \widehat{\bm{x}}\Vert_1\right] - \beta\mathbb{E}\left[D(\widehat{\bm{x}})\right] \\
\end{split}
\end{equation}
\begin{equation}
    \mathcal{L}_{D_1} = \mathbb{E}\left[D(\widehat{\bm{x}})\right] - \mathbb{E}\left[D(\bm{x})\right] + \lambda\mathbb{E}\left[\left({\Vert \nabla_{\widetilde{\bm{x}}} D(\widetilde{\bm{x}}) \Vert}_2 - 1\right)^2\right]  
\end{equation}
where $\widetilde{\bm{x}}=\epsilon\widehat{\bm{x}}_i+(1-\epsilon){\bm{x}},\ \epsilon\in(0,1)$. 
$\mathcal{L}_{\text{rec}}$ is defined as the data reconstruction loss.
$\mathcal{L}_{\text{adv}}$ denotes the adversarial loss. $\alpha, \beta, \lambda\geq0$ represent the weights of three loss functions. 
Significantly, the discriminator's loss function contains of a gradient penalty factor,
which promotes the stability of the adversarial training and reduces the risk of mode collapse \cite{wGANs}.

\textbf{Anomaly Inference.}
Let $\bm{\delta}_m^x \coloneqq \bm{x}_m - \widehat{\bm{x}}_m$, $\bm{\delta}_n^y \coloneqq \bm{y}_n - \widehat{\bm{y}}_n$
denote the reconstruction errors of normal and anomaly samples.
Motivated by the main idea that the reconstruction errors of anomaly cells are significantly larger than normal cells,
we hope to maximize the discrepancy of $\bm{\delta}_m^x$ and $\bm{\delta}_n^y$.
Here, we describe our objective with MMD (Maximum Mean Discrepancy), a non-parametric metric to quantify the difference between two probability distributions in a reproducing kernel Hilbert space (RKHS) \cite{kolouri2016sliced,gretton2012kernel}.
And the squared MMD between two sets of samples $\bm{x}_m \sim p$ and $\bm{y}_n \sim q$ can be derived as \cite{gretton2012kernel}:
\begin{equation}\label{eq:MMD}
    \begin{aligned}
        MMD^2\left(\bm{x}_m, \bm{y}_n\right) = & \Vert \frac{1}{m}\sum_{i=1}^{m}\phi(\bm{x}_i) -  \frac{1}{n}\sum_{j=1}^{n}\phi(\bm{y}_j)\Vert_{\mathcal{H}}^2 \\
        =  &\mathbb{E}_{\bm{x}, \bm{x}^{\prime} \sim p}\left[k(\bm{x}, \bm{x}^{\prime})\right] +
        \mathbb{E}_{\bm{y}, \bm{y}^{\prime} \sim q}\left[k(\bm{y}, \bm{y}^{\prime})\right] \\
        & - 2\mathbb{E}_{\bm{x} \sim p, \bm{y} \sim q}\left[k(\bm{x}, \bm{y})\right]
    \end{aligned}
\end{equation}
where $k$ is a postive definite kernel.
Therefore, the objective for detecting anomalous and normal cells can be expressed as follows:
\begin{equation}\label{eq:def}
        \min_{\bm{\delta}_m^x, \bm{\delta}_n^y} \quad \mathcal{L}_p =  -MMD^2\left(\bm{\delta}_m^x, \bm{\delta}_n^y\right)
\end{equation}
In fact, it indicates we need to optimize a mapping function $f_s: \mathbb{R}^p \rightarrow \{0, 1\}$ that partitions unlabeled total samples $\bm{\delta}_{m+n}$ into $\bm{\delta}_m^x$ and $\bm{\delta}_n^y$, aiming to minimize $\mathcal{L}_p$.
Therefore, we propose \Cref{th:opt_s} for transforming \eqref{eq:def}, and the proof is provided in Supplementary.
\begin{theorem}\label{th:opt_s}
Define $s_i \coloneqq f_s(\bm{\delta}_i)$. The optimization \eqref{eq:def} have an equivalent form:
\begin{equation}\label{eq:def_s}
    \min_{f_s} \quad \mathcal{L}_p = -\sum_{i}^{m+n}\sum_{j\neq i}^{m+n} k(\bm{\delta}_i, \bm{\delta}_j) \gamma(s_i, s_j)
\end{equation}
where
\begin{equation}\label{eq:gamma}
    \gamma(s_i, s_j) = 
    \begin{cases}
        \frac{1}{m(m-1)}, & s_i = s_j = 0 \\
        \frac{1}{n(n-1)}, & s_i = s_j = 1 \\
        \frac{-1}{mn}, & s_i \neq s_j
    \end{cases}
\end{equation}
If $s_i = 1$, sample $i$ will be classified as anomaly; otherwise, it will be classified as normal. 
\end{theorem}

For detecting anomalies, We aim to learn a predictor $f_p:\mathbb{R}^p \rightarrow [0, 1]$ to provide anomaly scores $p_i \coloneqq f_p(\bm{\delta}_i)$.
However, $\gamma(s_i, s_j): \{0, 1\} \times \{0, 1\} \rightarrow \mathbb{R}$ is discrete and doesn't exist gradients.
A natural idea is to extend $\gamma$ to the continuous scenario, resulting in the function $\gamma_c(p_i, p_j): (0, 1) \times (0, 1) \rightarrow \mathbb{R}$.
Here, we propose a potential $\gamma_c$ based on \Cref{th:gamma_c}, and the analytic properties of $\gamma_c$ everywhere indicate its strong smoothness, making it amenable to gradient descent optimization.
The proof of \Cref{th:gamma_c} is available at Supplementary.
\begin{theorem}\label{th:gamma_c}
    For any given $m, n$, there always exists $\gamma_c$ that extends $\gamma$ to the continuous scenario and satisfies:
    \begin{enumerate}[(1)]
        \item $\forall s_i, s_j \in \{0, 1\}$, $\gamma_c(s_i, s_j)=\gamma(s_i, s_j)$
        \item $\gamma_c$ is analytic everywhere within its domain.
    \end{enumerate}
    A potential $\gamma_c$ can be defined as follows:
    \begin{equation}\label{eq:gamma_c}
        \begin{aligned}
            \gamma_c(p_i, p_j) \coloneqq & \frac{\sin\pi p_i \sin\pi p_j}{\pi^2}\left(\frac{[m(m-1)]^{-1}}{p_ip_j} - \frac{(mn)^{-1}}{(p_i-1)p_j} \right.\\
            & \left.- \frac{(mn)^{-1}}{p_i(p_j-1)} + \frac{[n(n-1)]^{-1}}{(p_i-1)(p_j-1)}\right)
        \end{aligned}
    \end{equation}
\end{theorem}
Subsequently, the predictor can be optimized with the following loss function:
\begin{equation}\label{eq:loss_p}
    \mathcal{L}_p = -\sum_{i}\sum_{j\neq i} k(\bm{\delta}_i, \bm{\delta}_j) \gamma_c(p_i, p_j)
\end{equation}
Ultimately, by minimizing \eqref{eq:loss_p} through updating predictor with gradient descent, we can achieve our objective \eqref{eq:def} and obtain anomaly scores.

\textbf{Cross-domain Anomaly Detection.} 
Compared with other methods, ACsleuth not only measures the difference between anomalous and normal samples in a more effective manner,
enhancing the accuracy of anomaly detection, but also introduces transferability into this process.
Specifically, in the context of large sample sizes, even in the presence of batch effects or domain biases between the target and reference datasets,
we can still ensure that the accuracy of prediction remains unaffected. 
From a mathematical perspective, ACsleuth satisfies \Cref{th:trans}, which indicates the cross-domain transfer error can be bounded by sample size $n$.
In practice, when we minimize the objective \eqref{eq:def}, that is, when maximizing this batch-effect-independent $MMD^2\left(P_{\delta^{y*}}, P_{\delta^{x*}}\right)$.
Therefore, \Cref{th:trans} can indicate inherent transferability to realize cross-domain detection.
The proof of \Cref{th:trans} is provided in Supplementary.
\begin{theorem}\label{th:trans}
We assume the proportion of normal samples $\bm{x}_m$ to anomaly samples $\bm{y}_n$ satisfies $m/n = C \geq 1$, where $C$ is a constant but not large enough.
If \Cref{as:pattern} is valid and MMD is induced by linear kernels, for any samples $\bm{x}_m, \bm{y}_n$
there exist the biological information reconstruction errors $\bm{\delta}_n^{y*}, \bm{\delta}_m^{x*}$ that are independent of batch effects, such that their distributions $P_{\bm{\delta}^{y*}}, P_{\bm{\delta}^{x*}}$ satisfy:
\begin{multline}
    \mathbb{P}\left(\big| MMD^2(\bm{\delta}_m^x, \bm{\delta}_n^y) - MMD^2(P_{\bm{\delta}^{x*}}, P_{\bm{\delta}^{y*}}) \big| \geq \varepsilon \right) \leq \\
    \alpha \exp\left(-\frac{\beta C}{1+C}n\varepsilon^2\right)
\end{multline}
where $\varepsilon$ denotes transfer error, $\alpha, \beta$ are constants.
\end{theorem}
Based on \Cref{th:trans}, we can reconstruct normal samples in the reference scRNA-seq dataset 
and simply apply it on the target scATAC-seq dataset. 
This cross-domain task shows ACsleuth is capable of learning a robust representation of the entire cell thereby exhibiting strong 
generalization performance.

\subsection{Multi Sample Batch Effect Correction}
The identification and removal of confounding anomalies from target datasets empower 
ACsleuth to align multiple target datasets onto the feature space of the reference dataset, 
thus fulfilling the task of multi-sample batch correction for single-cell data. As 
illustrated in figure 1, this task contains two subtasks: (1) matching each normal sample 
in the target datasets with its most analogous counterpart in the reference dataset. A 
pair of such samples is termed a “kin” pair, indicating that the two paired samples are 
more likely to share identical biological contents. (2) learning a “style-divergence” 
matrix to represent the batch effects between target datasets and the reference dataset. 
This matrix can then be applied to map target datasets to the reference data space in a 
“style-transfer” manner.

Formally, we first define the batch effect in the context of gene expression analysis as: 
\begin{equation}
    x_{ij}={x_{ij}}^{'}+b_j+\epsilon_{ij}
\end{equation}
where $x_{ij}$ denotes the gene expression values for $j-th$ cell in the batch $i$, $b_{ij}$ 
is the batch effects of the batch $i$ and the term $\epsilon_{ij}~N(0,1)$ represents random noise, 
which is uneffected by batch effects.

For the first subtask, we introduce a GAN module (referred as \textit{module II}) that takes 
as input the embeddings of normal samples from both the reference and target datasets, 
which are generated by the encoder within \textit{module I}. The generator of this module is 
trained to learn a non-negative mapping matrix for generating samples in target 
datasets through those in the reference dataset, while the discriminator corresponding to 
learning to distinguish the authentic and generated samples. In detail, let $N_T$ 
and $N_R$ denote the number of samples in the target and reference dataset, then 
define $Z_T\in\mathbb{R}^{N_T\times p}$ as the embeddings of multi-sample target datasets, 
$Z_R\in\mathbb{R}^{N_R\times p}$ the embeddings of the reference dataset, 
$M\in\mathbb{R}^{N_T\times N_R}$ the trainable non-negative mapping matrix. The generated 
sample represents as: 
\begin{equation}
    {\widehat{Z}}_T=ReLU(M)Z_R 
\end{equation}
The rectified linear unit (ReLU) function serves as the non-negative constraint on $M$. 
Furthermore, we indicate the loss functions for the generator and the discriminator of 
\textit{module II} as:
\begin{equation}
    \mathcal{L}_{\text{Gen}_2} = \alpha\mathbb{E}\|Z_T - \widehat{Z}_T\|_1 - \beta\mathbb{E}[D(\widehat{Z}_T)] 
\end{equation}
\begin{equation}
        \mathcal{L}_{D_2} = D(\widehat{Z}_T) - \mathbb{E}\left[D(Z_T)\right] + \lambda\mathbb{E}\left[({\|\nabla D(\widetilde{Z}_T)\|}_2 - 1)^2\right]
\end{equation}
where  $\widetilde{Z}=\epsilon\widehat{Z}+\left(1-\epsilon\right)Z,\ 
\epsilon\in\left(0,1\right)$.$\alpha,\beta,\lambda\geq0$ represent the weights of the loss 
terms. Upon the completion of the adversarial training of \textit{module II}, the column index of 
the maximum value on the $i^{th}$ row of matrix M corresponds to the index of the reference 
sample that forms a “kin” pair with the $i^{th}$ sample in the target dataset. 
We propose a hypothesis that samples belonging to the same “kin” pair share analogous 
biological content, allowing us to approximate the reference sample by eliminating 
the “style-divergence” from the target sample. Therefore, for the second subtask, a 
GAN for “style-transfer” (referred as \textit{module III}) is employed to learn the 
“style-divergences”, or namely batch effects, between pairs of the target and reference 
datasets as rows of a trainable matrix $S\in\mathbb{R}^{N_{batch}\times p}$ in the latent 
embedding space. In detail, for an sample i in the target datasets, the encoder of 
the generator within \textit{module III} maps its raw gene expression vector 
$x_i$ to $z_i\in\mathbb{R}^p$ in the latent embedding space. This latent representation is 
then employed to estimate the embeddings of its “kin” sample $j$ as follows:
\begin{equation}
z_i=f_{\textit{MLP}}(x_i,W_5), {\widehat{z}}_j = z_i-S^T b_i
\end{equation}
where $b_i\in\mathbb{R}^{N_{batch}}$ denotes {i}'s one-hot encoded batch-identity vector for 
selecting the corresponding “style-divergence” row from $S$. In sequence, the decoder of the 
generator maps ${\widehat{z}}_j$ back to the original reference data space as ${\widehat{x}}_j$. The 
discriminator of \textit{module III} is simply trained to distinguish the $x_j$ and ${\widehat{x}}_j$. We 
indicate the loss functions for the generator and the discriminator of \textit{module III} as:
\begin{equation}
\mathcal{L}_{Gen_3}=\alpha\mathbb{E}{||x_R- \widehat{x}_R||}_1-\beta\mathbb{E}[D(\widehat{x}_R)]
\end{equation}
\begin{equation}
\mathcal{L}_{D_3}=\mathbb{E}\left[D\left(\widehat{x}_R\right)\right]-\mathbb{E}\left[D\left(x_R\right)\right]+\lambda\mathbb{E}[{({||\nabla D(\widetilde{x}_R)||}_2-1)}^2]
\end{equation}
where $\alpha,\beta,\lambda$ and $\widetilde{x}_R$ have the same definitions as 
their counterparts in \textit{module II}. Finally, samples in multiple target datasets can be 
batch-corrected and aligned in the common reference data space by passing through the 
trained generator within \textit{module III}.
\subsection{Fine-grained Anomalous Cell Detection}
\begin{table}
    \centering
    \begin{tabular}{cccccc}
        \hline
        DATA  & Type & Genes  & Cells & Anomaly Ratio \\
        \hline
        \multirow{2}{*}{\centering PBMCs} & \multirow{2}{*}{\centering scRNA-seq} & \multirow{2}{*}{\centering 32738}  & 3684 & 13.14\%  \\
        & & & 3253 & 12.73\% \\ 
        \multirow{2}{*}{\centering Cancer} & \multirow{2}{*}{\centering scRNA-seq} & \multirow{2}{*}{\centering 33538} & 7721 & 50.03\%  \\
        & & & 4950 & 58.12\% \\ 
        cSCC & scRNA-seq & 32738 & 6181 & 37.58\% \\
        TME & scATAC-seq & 23127 & 2968 & 60.24\% \\
        \hline
    \end{tabular}
    \caption{Information of Datasets.}
    \label{tab:tab1}
\end{table}
\begin{table*}[ht]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{cccccccccc}
        \hline
        DATA & Anomaly & Highly Genes & ACsleuth & SLAD & ICL & NeuTraL & RCA & AE & Scmap \\
        \hline
        \multirow{6}{*}{\centering PBMCs} & \multirow{3}{*}{\centering B Cells} & 3000 & \textbf{0.833$\pm$0.034} & 0.347$\pm$0.023 & 0.262$\pm$0.029 &0.299$\pm$0.014 & 0.267$\pm$0.007 & 0.449$\pm$0.015 & 0.377$\pm$0. \\
        & & 6000 & \textbf{0.818$\pm$0047} & 0.279$\pm$0.021 & 0.246$\pm$0.024 & 0.234$\pm$0.022 & 0.266$\pm$0.009 & 0.401$\pm$0.008 & 0.393$\pm$0. \\
        & & full & \textbf{0.785$\pm$0.044} & 0.149$\pm$0.007 & 0.176$\pm$0.014 & 0.215$\pm$0.010 & 0.266$\pm$0.006 & 0.258$\pm$0.006 & 0.390$\pm$0. \\
        & \multirow{3}{*}{\centering NK Cells} & 3000 & \textbf{0.804$\pm$0.004} & 0.548$\pm$0.013 & 0.307$\pm$0.042 & 0.330$\pm$0.025 & 0.258$\pm$0.025 & 0.462$\pm$0.012 & 0.495$\pm$0. \\
        & & 6000 & \textbf{0.819$\pm$0.022} & 0.587$\pm$0.021 & 0.359$\pm$0.038 & 0.351$\pm$0.030 & 0.259$\pm$0.024 & 0.545$\pm$0.006 & 0.615$\pm$0. \\
        & & full & \textbf{0.757$\pm$0.035} & 0.607$\pm$0.015 & 0.168$\pm$0.013 & 0.294$\pm$0.065 & 0.259$\pm$0.024 & 0.617$\pm$0.005 & 0.589$\pm$0. \\ 
        \hline
        \multirow{6}{*}{\centering Cancer-Single} & \multirow{3}{*}{\centering Epithelial \& Immune Tumor} & 3000 & \textbf{0.926$\pm$0.009} & 0.881$\pm$0.006 & 0.856$\pm$0.006 & 0.889$\pm$0.011 & 0.818$\pm$0.033 & 0.896$\pm$0.002 & 0.679$\pm$0. \\
        & & 6000 & 0.832$\pm$0.014 & \textbf{0.862$\pm$0.004} & 0.844$\pm$0.005 & 0.847$\pm$0.006 & 0.600$\pm$0.041 & 0.853$\pm$0.003 & 0.683$\pm$0. \\
        & & full & \textbf{0.826$\pm$0.035} & 0.785$\pm$0.005 & 0.572$\pm$0.006 & 0.691$\pm$0.040 & 0.649$\pm$0.009 & 0.717$\pm$0.006 & 0.734$\pm$0. \\
        & \multirow{3}{*}{\centering Epithelial \& Stromal Tumor} & 3000 & 0.860$\pm$0.049 & 0.888$\pm$0.014 & 0.899$\pm$0.018 & \textbf{0.913$\pm$0.008} & 0.677$\pm$0.036 & 0.746$\pm$0.005 & 0.688$\pm$0. \\
        & & 6000 & 0.824$\pm$0.007 & 0.828$\pm$0.014 & 0.856$\pm$0.014 & \textbf{0.892$\pm$0.008} & 0.543$\pm$0.025 & 0.681$\pm$0.006 & 0.735$\pm$0. \\
        & & full & \textbf{0.717$\pm$0.059} & 0.638$\pm$0.005 & 0.556$\pm$0.006 & 0.686$\pm$0.024 & 0.471$\pm$0.026 & 0.571$\pm$0.003 & 0.683$\pm$0. \\
        \hline
        \multirow{3}{*}{\centering TME} & \multirow{3}{*}{\centering Tumor(pre)} & 3000 & 0.0$\pm$0.0 & 0.582$\pm$0.011 & 0.791$\pm$0.050 & 0.578$\pm$0.016 & 0.493$\pm$0.030 & 0.488$\pm$0.001 & 0.752$\pm$0. \\
        & & 6000 & 0.0$\pm$0.0 & 0.590$\pm$0.017 & 0.728$\pm$0.028 & 0.591$\pm$0.013 & 0.511$\pm$0.038 & 0.509$\pm$0.002 & 0.733$\pm$0. \\
        & & full & 0.0$\pm$0.0 & 0.640$\pm$0.021 & 0.756$\pm$0.012 & 0.638$\pm$0.017 & 0.538$\pm$0.032 & 0.532$\pm$0.002 & 0.529$\pm$0. \\
        \hline
        \multirow{3}{*}{\centering cSCC} & \multirow{3}{*}{\centering Diff \& Basal \& Cyc Tumor} & 3000 & 0.0$\pm$0.0 & 0.486$\pm$0.014 & 0.173$\pm$0.025 & 0.085$\pm$0.042 & 0.0$\pm$0.0 & 0.484$\pm$0.004 & 0.0$\pm$0. \\
        & & 6000 & 0.0$\pm$0.0 & 0.503$\pm$0.007 & 0.154$\pm$0.020 & 0.123$\pm$0.030 & 0.0$\pm$0.0 & 0.491$\pm$0.006 & 0.0$\pm$0. \\
        & & full & 0.0$\pm$0.0 & 0.510$\pm$0.010 & 0.155$\pm$0.052 & 0.140$\pm$0.031 & 0.0$\pm$0.0 & 0.561$\pm$0.005 & 0.0$\pm$0. \\
        \hline
        \multirow{3}{*}{\centering PBMC-TME} & \multirow{3}{*}{\centering Tumor(pre)} & 3000 & & 0.570$\pm$0.021 & 0.583$\pm$0.032 & 0.645$\pm$0.044 & 0.$\pm$0. & 0.524$\pm$0.002 & 0.$\pm$0.  \\
        & & 6000 & & 0.573$\pm$0.034 & 0.629$\pm$0.044 & 0.637$\pm$0.070 & 0.$\pm$0. & 0.533$\pm$0.001 & 0.$\pm$0. \\
        & & full & & 0.562$\pm$0.019 & 0.637$\pm$0.031 & 0.664$\pm$0.054 & 0.$\pm$0. & 0.524$\pm$0.001 & 0.$\pm$0. \\
        \hline
    \end{tabular}
    }
    \caption{Average F1 score with standard deviation for anomaly detection on single-cell transcriptomics datasets.}
    \label{tab:tab2}
\end{table*}

\begin{table*}
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{ccccccccc}
        \hline
        DATA  & Anomaly & Highly Genes & ACsleuth & DFCN & EDESC & scTAG & Leiden & K-Means \\
        \hline
        \multirow{6}{*}{\centering Cancer-Single} & \multirow{3}{*}{\centering Epithelial \& Immune Tumor} & 3000 & 0.0$\pm$0.0 & 0.713$\pm$0.015 & 0.606$\pm$0.283 & 0.528$\pm$0.050 & 0.0$\pm$0.0 & 0.798$\pm$0.011 \\
        & & 6000 & 0.0$\pm$0.0 & 0.652$\pm$0.020 & 0.782$\pm$0.041 & 0.316$\pm$0.117 & 0.0$\pm$0.0 & 0.778$\pm$0.014 \\
        & & full & 0.0$\pm$0.0 & 0.628$\pm$0.025 & 0.751$\pm$0.004 & OOM & 0.0$\pm$0.0 & 0.748$\pm$0.004 \\
        & \multirow{3}{*}{\centering Epithelial \& Stromal Tumor} & 3000 & 0.0$\pm$0.0 & 0.493$\pm$0.149 & 0.638$\pm$0.053 & 0.599$\pm$0.009 & 0.0$\pm$0.0 & 0.615$\pm$0.014 \\
        & & 6000 & 0.0$\pm$0.0 & 0.274$\pm$0.034 & 0.593$\pm$0.066 & 0.543$\pm$0.082 & 0.0$\pm$0.0 & 0.611$\pm$0.026 \\
        & & full & 0.0$\pm$0.0 & 0.0$\pm$0.0 & 0.474$\pm$0.037 & OOM & 0.0$\pm$0.0 & 0.489$\pm$0.010 \\
        \multirow{3}{*}{\centering cSCC} & \multirow{3}{*}{\centering Diff \& Basal \& Cyc Tumor} & 3000 & 0.0$\pm$0.0 & 0.376$\pm$0.020 & 0.362$\pm$0.143 & 0.227$\pm$0.065 & 0.0$\pm$0.0 & 0.104$\pm$0.006 \\
        & & 6000 & 0.0$\pm$0.0 & 0.355$\pm$0.002 & 0.395$\pm$0.013 & 0.152$\pm$0.015 & 0.0$\pm$0.0 & 0.107$\pm$0.003 \\
        & & full & 0.0$\pm$0.0 & 0.366$\pm$0.080 & 0.291$\pm$0.024 & OOM & 0.0$\pm$0.0 & 0.0$\pm$0.0 \\
        \multirow{3}{*}{\centering Cancer-Multi} & \multirow{3}{*}{\centering Epithelial \& Immune \& Stromal Tumor} & 3000 & 0.0$\pm$0.0 & 0.376$\pm$0.020 & 0.362$\pm$0.143 & 0.227$\pm$0.065 & 0.0$\pm$0.0 & 0.104$\pm$0.006 \\
        & & 6000 & 0.0$\pm$0.0 & 0.355$\pm$0.002 & 0.395$\pm$0.013 & 0.152$\pm$0.015 & 0.0$\pm$0.0 & 0.107$\pm$0.003 \\
        & & full & 0.0$\pm$0.0 & 0.366$\pm$0.080 & 0.291$\pm$0.024 & OOM & 0.0$\pm$0.0 & 0.0$\pm$0.0 \\
        \hline
    \end{tabular}
    }
    \caption{Average F1*NMI score with standard deviation for fine-grained anomaly detection on single-cell transcriptomics datasets.}
    \label{tab:tab3}
\end{table*}

In this section, we introduce how ACsleuth subtypes anomaly cells detected by preceding 
modules. Since \textit{module III} can align anomalies identified by module I across multiple 
target datasets in the common reference space, thus significantly mitigating the 
confounding batch variations and facilitating the anomaly subtyping task. Specifically, we 
combine the embeddings and reconstruction loss generated by \textit{module I} to distinguish 
between various novel cell subtypes and address the variations among them. We first give 
some notations: for an anomalous cell $i$, ${x_i}^g$ denotes 
cell’s batch-corrected gene expression vector, and 
${\widehat{x}_i}^g$ the gene expression vector reconstructed by 
\textit{module I}, respectively. ${r_i}^g$ represents the reconstruction 
loss of gene expressions. $z_i$ and $\zeta_i$ represent the embeddings of 
$x_i$ and $r_i$, which are also generated by the 
encoder within \textit{module I}. Above terms are calculated as follows:
\begin{equation}
{r_i}^g = {x_i}^g-{{\widehat{x}}_i}^g, r_i = {r_i}^g
\end{equation}
\begin{equation}
    \begin{split}
        z_i = f_{\textit{MLP}}({x_i}^g,W_1),{z_i}^\ast = TF([z_i||\zeta_i],W_{tf}), 
    \end{split}
\end{equation}
\[
   where \zeta_i = f_{\textit{MLP}}({r_i}^g,W_1)    
\]
where ${z_i}^\ast$ represents the fused embeddings and reconstruction 
loss of anomalous cell $i$ by a transformer fusion (TF) block. We then leverage a 
discriminatively boosted clustering algorithm, DESC \cite{DESC}, to cluster 
anomalies according to their  $z^\ast$. In detail, DESC applies a Cauchy kernel 
to compute the soft assignment score of an anomaly cell i to a cluster $j$ as:
\begin{equation}
    q_{i,j} = \frac{{{(1 + \left\| z_i^\ast - \mu_j \right\|^2 / v)}}^{-1}}{{\sum_{j'} \left(1 + \left\| z_i^\ast - \mu_{j'} \right\|^2 / v\right)^{-1}}}
\end{equation}
where $q_{i,j}$ represents the probability that cell $i$ belongs to cluster $j
, \mu_j$ the centroid of cluster $j, v$ the degree of freedom of the Cauchy kernel. 
The clustering loss function is a KL-divergence $\mathcal{L}$ calculated on $q$ and an 
auxiliary target distribution $p$, defined as:
\begin{equation}
p_{i,j}=\frac{q_{i,j}^2/\sum_{i} q_{i,j}}{\sum_{j}\left(q_{i,j}^2/\sum_{i} q_{i,j}\right)} 
\end{equation}
\begin{equation}
    \mathcal{L}=\sum_{i}{\sum_{j} p_{i,j}log\left(\frac{p_{i,j}}{q_{i,j}}\right)}
\end{equation}
Note that anomalies with a high-confident assignment are overweighted in p. 
Experimentally, the iterative updating of TF weights $W_{TF}$ and cluster 
centroid $\mu$ with the objective of minimizing $\mathcal{L}$ drives q to p, 
resulting in a gradual transformation of harder-to-cluster embeddings 
$z^\ast$ into easier ones. The self-paced and iterative DESC persists until the 
change in the hard assignments of anomalous cells falls below a predefined threshold or 
reaches a predefined number of iterations. The subtype labels of anomalies can be readily 
obtained from their final hard cluster assignments. Additionally, the number of subtype 
labels is assumed to beknown or automatically inferred during the clustering. In detail, 

\section{Experiments}
\subsection{Experimental Settings}
\textbf{Dataset.}	Our experiments contain anomaly cells’ detection and anomaly cell 
clustering. Furthermore, each task splits to two parts. For the former, we use datasets 
simply called PBMCs, Cancer, cSCC and TME, including scRNA-seq and scATAC-seq, as we have both 
intra-data detection and cross-data detection. In detail, we specifically treat B cells 
and natural killer (NK) cells as anomalies in PBMCs. For the latter, we use Cancer and cSCC for 
the task of single-batch and concatenate different batches for the multi-batch task. The 
detailed information is described in Table 1. Each dataset is split according to the 
number of highly variable genes, including 3000, 6000 and all genes respectively. Besides, 
we normalize and log the data using the Scanpy \cite{scanpy} package.

\textbf{Baselines.} For the anomaly detection, we compare ACsleuth with four recently 
state-of-the-art anomaly detection methods in tabular data (SLAD \cite{SLAD}, 
ICL \cite{ICL}, NeuTraL \cite{NeuTraL}, RCA \cite{RCA}) and two 
classical methods (Scmap \cite{scmap} and AE \cite{AE}). The code 
we use is provided by authors, and each one we use default hyperparameters if no specific 
instructions are given.

For the fine-grained anomalous cell detection, we compare ACsleuth with the combination of one baseline of 
anomaly detection and the state-of-the-art method of clustering. The former we choose the 
SLAD, as it has better performance both in the ability of both detecting anomalies and 
computational efficiency than other baselines. Then select four recent clustering methods 
(scTAG \cite{scTAG}, EDESC \cite{EDESC}, DFCN \cite{DFCN}, Leiden \cite{leiden} 
and K-means \cite{kmeans}), including deep embedded clustering and scRNA-seq 
clustering, and one classical clustering method. 

\textbf{Evaluation Metrics.}	We assess the performance with two widely used evaluation metrics: 
F1 score for anomaly detection, normalized mutual information (NMI) for cell clustering. 
For the cell’s subtype detection, we simply multiply the F1 score and the NMI to create 
a new metric, which is used to measure the effect of the whole task. The higher the values 
of these metrics, the better the detecting and the clustering performance. The reported 
metrics are averaged results with standard deviations over ten independent runs.

\textbf{Implementation Details.} 	Our experiments are conducted with a 
NVIDIA GeForce RTX 3090 GPU and 24GB of memory.  (Hyperparameters settings)

\subsection{Results}
\textbf{Anomaly detection results.}	Table 1 illustrates the detection performance in terms of 
F1 score, of our model ACsleuth and the competing methods. Each dataset’s best detector is 
boldfaced. On the three datasets with different number of highly variable genes, we totally 

\textbf{Anomaly detection Cross-domain results.}	To evaluate the generalization capacity of ACsleuth 
in anomaly detection, we trained the model on the PBMCs then tested it on the TME. The 
former dataset is the scRNA-seq data, while the latter is the scATAC-seq data, both 
representing different domains for the same cells. Specifically, we screen out genes 
common to both datasets, then still split them according to the number of highly variable 
genes. The performance of ACsleuth and baseline methods illustrate in the bottom of Table 1. Results 
demonstrate that ACsleuth outperforms baseline anomaly detection methods, which means it can 
construct robust representations for each single-cell.

\textbf{Anomalies fine-grained detection results.}	We evaluate ACsleuth by comparing it with simple combinations 
of anomaly detection methods and clustering methods for the fine-grained anomaly detection task. In 
this section, we assess the impact of fine-grained detection task in two scenarios: the 
single-batch representing a common condition and multi-batch where different batches of 
Cancer datasets are concatenated to emphasize the importance of batch effect correction 
in scRNA-seq data. As shown in Table 2, we can conclude that for the single-batch task, 
ACsleuth consistently achieves stable and superior performance, as reflected by the product 
of F1 scores for detecting anomalies and NMI scores for clustering on most of cases. In 
the multi-batch task, ACsleuth significantly outperforms baseline methods. Results fully 
demonstrate that batch effects are intractable for recent methods. ACsleuth addresses this 
challenge by learning such kind of biases from the normal samples split from anomaly 
detection, then subsequently correcting them on the detected anomalies before subtyping 
them. We empirically establish the necessity of the batch correction module for the scRNA-seq data analysis.

\subsection{Ablation Studies}
In this section, we conduct a series of ablation studies to validate how different 
configurations of each module impact the anomaly subtyping task. We still assess ACsleuth with 
the Cancer dataset since it contains more genes (features) and cells (samples). We 
present experiments sequentially for anomaly detection, batch effect correction and anomaly 
subtyping as outlined below.

\textbf{Effect of memory bank for anomaly detection.}	As introduced in section 3.1, 
we employ a memory block to enhance the training of the anomaly detector. In particular, 
we evaluate ACsleuth without the memory block in the anomaly detection module, and results are 
illustrated in Figure 2. Upon the removal of the memory block, the anomaly detection 
module easily collapses, significantly impairing affect the subsequent subtyping task.

\textbf{Essence of the batch correction module.}	We have discussed that batch effect correction 
is imperative for the scRNA-seq data analysis. It is straightforward to question whether 
the subtyping results will be worse if we directly subtype anomalies samples after 
detecting them, following the same settings as baseline methods. As shown in figure 2, 
the direct connection of two tasks is considerably adverse compared with the original ACsleuth.

\textbf{Validity of the reconstruction error in the fine-grained detection task.}	At last, we carry out an 
ablation study for the input of the fine-grained anomaly detection module. The 
reconstruction error generated from anomaly detection module (\textit{Module I}) serves as 
anomalous scores leveraged to identify anomalies in \textit{Module I}, encompassing similarity 
information among them. If we merely consider the original gene expression matrix as the 
input of subtype clustering, there will be an underutilization of information regarding 
anomalous cells, as empirically demonstrated in figure 2.
\section{Conclusion}
In this paper, we innovatively propose ACsleuth, a comprehensive workflow that sequentially 
integrates anomaly detection with fine-grained anomaly detection, which further contains a batch 
correction module to alleviate bias inherent in the raw data. More precisely, we first 
employ a GAN-based model to detect anomalies according to the reconstruction error, then 
capture batch effects from filtered normal samples and correct them on the detected 
anomalies. We also prove that the domain adaptation module is effective for cross-domain tasks. 
Finally, we utilize the DESC clustering algorithm to fine-grained detecting anomalous cells. 
ACsleuth outperforms state-of-the-art approaches in anomaly detection methods for tabular 
data and their simple combinations with deep clustering algorithms. Empirically results 
strongly improve ACsleuth’ superiority, demonstrating its generalizability and robustness. 
We’d like to further explore the potential of the domain adaptative fine-grained anomaly 
detection workflow for tabular data generalized to another fields.
\section*{Acknowledgments}




%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{ijcai24}

\end{document}

